# 日期: 2025-12-26 06:40:43
# 音檔: 588.1 秒 (9.8 分鐘)
# 處理: 60.8 秒
# 速度: 9.7x 即時

好那我目前用的神經網路的架構主要是右邊這張大圖有分 value network 跟 policy network右邊的話是 value network 左邊的話是 policy那 value network 主要就是用來判斷在這個環境下它使用哪個 action 會得到怎樣的獎勵如果以我的實際情況就是我 base station 跟 uav 的位置移植的情況下移植他們中間就會有通道嘛那如果在這個通道下我打某個波數的話它會得到多少的 reward那這個 reward 是訊號通常是訊號在我這邊就只訊號強度啦那因為 base station 跟跟 UAV 的位置你打不同的波速它得到的訊號強度就會不一樣你打越歪它得你打越歪 UAV 接收到的訊號強度自然就會越差你打越準 UAV 得到就會越強所以 value network 就是用來判斷這件事情的那 policy network那左邊 policy network 是決定動作啊以我的情況就是我 base station 跟跟 uav 的位置在指導情況下那我 policy network 我要決定我打哪個波數因為實際上打哪個波數都可以但唯一差別就是說它訊號強度會差因為訊號強度很強所以訊號程度很差嘛那 policy 的決定因素就會根據說那個你的 value network 來決定因為我要透過 value我要透過 value network 我知道說我打哪個方向會強所以我 policy 才會決定說我要打這個方向所以說在訓練方式基本上就是這整個演算法訓練方式就是我環境我已經知道相關資訊了環境會進去網路然後包含說一個我看到我可能掃過的 rsrpr 還有包含有哪些播出可以使用等等等等的這些環境資訊都丟進去這些環境資訊用神經網路去然後神經網路根據這些環境資訊去學習然後去學習之後它分別有 policy network 跟 value network 的訓練那 policy network 它就是基於現在已經有的現在已經有架構的 value network有 value network 架構可能現在有一個初始值然後用這個初始值去判斷說我這個 value我怎麼決定波數然後跟每個波數它的值多少然後等到收集完相當量的值就是我 policy 會因更新過好幾次我決定很多值然後根據這些值的決定情況我再去統整說我現在這個 value 判斷的是不是是不是對的是不是錯的應該不能講是不是對是不錯對錯是穩在看但以神經網路的角度來看就是我這個 barrier network 觀測出來的結果是不是跟是不是那個需要修正啊那所以 policy 多次訓練完之後才會去訓練一次 barrier network 去校正然後 barrier network 因為更新完之後它會有一個新的 model然後用新的再去訓練 policy然後這樣重複重複所以說那個所以說你在過程所以說過程中你那個 better network 會越來理論上會訓練得越來越好如果你訓練的方向是對的訓練得越來越好那你訓練得越來越好那你這個 policy 判斷就準確所以 policy 得到的學習 action 也會越好所以最後如果順利狩獵的話你這個神經網路會學到跟理論的情況一樣好的結果就是我最理想的情況就是 batch 的訊號在這邊然後 uav 在這邊然後我看到這個情況就知道說我要打這個然後因為 uav 會移動然後我也可以透過他的就透過資料訓練所以我知道說他移動軌跡大概會這樣子所以我也可以同樣去預判說我接下來的時間因為這是未來的進行未來進行的話你機器也不知道說未來進行會如何但是你如果經過就經過訓練你就你大概會知道說它大概變化是這個樣子所以未來有可能會是這個樣子所以說我就知道說我下個時間可以達到多數那這個是演算法的部分演算法的大概部分那整體框架的話細部是像這樣子描述的那細部的話你可以看到說我前面在訓練的時候因為它在做 beam tracking 的情況是有時間的相關性就是你上個時間跟下個時間它的位置相關是有連續性的對吧因為 he will be 它的移動是連續移動那連續移動的話如果你要考慮時間相關性的話就在機器學習裡面有一個工具就叫 lsdm那 lsdm 就是用來訓練這種動態情況然後找到一個可以方便我們找到一個規律的移動變化所以我們會先把先把相關的資訊就環境可以得到的資訊先通過 lsn 先濾變那 lsn 濾變你就可以得到它時間相關性那空間的相關性的話就是再經過一次那個 cnn就是所謂的 convolutional neural network 去濾出它的空間相關性所以這樣有時間有空間相關性就可以知道說它的特徵啊那知道它的特徵之後我這邊就已經保有它那個時間跟空間的連續性跟相關性所以用這個去丟進去我的 value network那 value network 就包含動作 action 跟剛剛這個 statestate 就是等於說我現在這個位置的資訊的情形我現在這個位置的資訊它的相關的空間時間特徵是什麼以這個空間時間特徵下的條件然後再給它一個特定的 action 就是所謂的 reward就是所謂 better network reward所以就可以得到一個判斷機制那這個判斷依據就可以拿去當做 policy network 更新資訊那 policy network 的 input 就只有那個feature就只有 feature 了因為就是這個環境藝術我要去得到說我現在應該要做的 bin selection 是什麼所以主要在 react 需要傳的可能就大概就會這個啦bin selection 就是所謂的 bin idea我們選哪一個 bin然後選哪一個 bin 打出去匯出去給那個記憶台然後記憶台然後記憶才再傳到另外一個傳到另一處自己然後再傳到另外一個地方去呈現這部分就跟剛剛松尼學長呈現的圖是一致的這個是大概 beam tracking 的演算法您說他一開始要收的資料有哪些我們的環境可以收的資料就包含那個ISOP基本上最好是因為要做 tracking 最好是 CSI但如果真的沒有辦法就只能選 ISOP那模擬情況是 CSI但這個部分要根據你後面真的能夠收到的資料有哪些才去決定那你目前程式設計它是吃什麼檔案吃什麼的格式呢我目前的資料 database 都是用 mat dotmadeable對啊就是用 shona 產生完的通道然後存成 mat dot然後存成 mat dot 讓它讀了解好應該先這樣子