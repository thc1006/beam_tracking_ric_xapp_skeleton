# -*- coding: utf-8 -*-
"""Sionna_Beam_Tracking_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Skub0UErkcl9bV-VxRU0v3ALiAKJWyXr
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random

# ===== System Parameters =====
TX_ANT = 8 * 4          # TX antennas
RX_ANT = 1              # RX antennas
N_SC = 273               # subcarriers
N_BEAM = 77              # total beams

# Candidate beams
K = 5                   # candidate size
T_LSTM = 5              # temporal window

# ===== Beam Codebook =====
phi_list = torch.arange(-60, 61, 12)      # 11
theta_list = torch.arange(-60, 61, 20)    # 7

beam_angles = []
for phi in phi_list:
    for theta in theta_list:
        beam_angles.append([phi.item(), theta.item()])

beam_angles = torch.tensor(beam_angles)   # [77, 2]

def beam_id_to_angle(beam_id: torch.LongTensor):
    """
    beam_id: (...,) in [1, 77]
    return: (..., 2) -> (phi, theta)
    """
    return beam_angles[beam_id - 1]

class BeamCandidateSet:
    def __init__(self, beam_angles, k=5):
        self.beam_angles = beam_angles
        self.k = k

    def get_candidates(self, main_beam_id):
        """
        main_beam_id: int (1~77)
        return: List[int] (beam ids)
        """
        idx = main_beam_id - 1
        ref = self.beam_angles[idx]

        dist = torch.norm(self.beam_angles - ref, dim=1)
        nearest = torch.topk(dist, k=self.k, largest=False).indices

        return (nearest + 1).tolist()

class CSITemporalEncoder(nn.Module):
    def __init__(self, tx_dim=32, hidden_dim=32, num_layers=1):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=tx_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

    def forward(self, csi_seq):
        """
        csi_seq: [B, T, N_sc=273, TX=32]
        return:  [B, 273, hidden_dim]
        """
        B, T, N_sc, TX = csi_seq.shape

        # 把 subcarrier 當 batch 跑 LSTM
        x = csi_seq.permute(0, 2, 1, 3)        # [B, 273, T, 32]
        x = x.reshape(B * N_sc, T, TX)         # [B*273, T, 32]

        out, _ = self.lstm(x)                   # [B*273, T, 32]

        # 取最後時間點
        out = out[:, -1, :]                    # [B*273, 32]
        out = out.view(B, N_sc, -1)            # [B, 273, 32]

        return out


class CSIFrequencyEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.conv = nn.Sequential(
            nn.Conv1d(32, 12, kernel_size=5, stride=3),
            nn.ReLU(),

            nn.Conv1d(12, 24, kernel_size=5, stride=3),
            nn.ReLU(),

            nn.Conv1d(24, 48, kernel_size=5, stride=3),
            nn.ReLU()
        )

        self.out_dim = 48 * 9

    def forward(self, x):
        """
        x: [B, 273, 32]
        return: [B, 432]
        """
        x = x.permute(0, 2, 1)   # [B, 32, 273]
        x = self.conv(x)         # [B, 48, 9]
        x = x.flatten(start_dim=1)
        return x

class CSIEncoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.temporal = CSITemporalEncoder(
            tx_dim=32,
            hidden_dim=32
        )

        self.freq = CSIFrequencyEncoder()

        self.fc = nn.Sequential(
            nn.Linear(432, 128),
            nn.ReLU()
        )

    def forward(self, csi_seq):
        """
        csi_seq: [B, T, 273, 32]
        return:  [B, 128]
        """
        x = self.temporal(csi_seq)   # [B, 273, 32]
        x = self.freq(x)             # [B, 432]
        x = self.fc(x)               # [B, 128]
        return x


class BeamIDEncoder(nn.Module):
    def __init__(self, num_beams=77, embed_dim=16):
        super().__init__()
        self.embedding = nn.Embedding(num_beams + 1, embed_dim)

    def forward(self, beam_id):
        return self.embedding(beam_id)

class BeamTrackingFeatureEncoder(nn.Module):
    def __init__(self, k=5):
        super().__init__()

        self.csi_encoder = CSIEncoder()
        self.beam_encoder = BeamIDEncoder(N_BEAM, 16)

        self.fc = nn.Sequential(
            nn.Linear(1 + 128 * (1 + k) + 16 * (1 + k), 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )

        self.k = k

    def forward(
        self,
        rss_main,          # [B, 1]
        csi_main,          # [B, T, 273, 32]
        candidate_ids,     # [B, K]
        candidate_csi,     # [B, K, T, 273, 32]
        prev_beam_id       # [B]
    ):
        B = rss_main.size(0)

        main_csi_feat = self.csi_encoder(csi_main)
        main_beam_feat = self.beam_encoder(prev_beam_id)

        cand_csi_feat = []
        for k in range(self.k):
            cand_csi_feat.append(
                self.csi_encoder(candidate_csi[:, k])
            )
        cand_csi_feat = torch.cat(cand_csi_feat, dim=1)

        cand_beam_feat = self.beam_encoder(candidate_ids).view(B, -1)

        x = torch.cat([
            rss_main,
            main_csi_feat,
            cand_csi_feat,
            main_beam_feat,
            cand_beam_feat
        ], dim=1)

        return self.fc(x)

class BeamTrackingBackbone(nn.Module):
    def __init__(self, feature_dim=128, hidden_dim=128):
        super().__init__()

        self.lstm = nn.LSTM(
            feature_dim,
            hidden_dim,
            batch_first=True
        )

    def forward(self, x, hidden=None):
        """
        x: [B, T, 128]
        """
        out, hidden = self.lstm(x, hidden)
        return out[:, -1], hidden

class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.ptr = 0

    def push(self, obs, action, reward, next_obs, done):
        data = (obs, action, reward, next_obs, done)
        if len(self.buffer) < self.capacity:
            self.buffer.append(data)
        else:
            self.buffer[self.ptr] = data
        self.ptr = (self.ptr + 1) % self.capacity

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)

        obs, action, reward, next_obs, done = zip(*batch)

        # 假設 obs 是 dict（強烈建議）
        obs_batch = {
            k: torch.stack([o[k] for o in obs])
            for k in obs[0].keys()
        }

        next_obs_batch = {
            k: torch.stack([o[k] for o in next_obs])
            for k in next_obs[0].keys()
        }

        return (
            obs_batch,
            torch.stack(action),
            torch.stack(reward),
            next_obs_batch,
            torch.stack(done)
        )

class PolicyNetwork(nn.Module):
    def __init__(self, cfg: BeamTrackingFeatureEncoder):
        super().__init__()

        self.csi_encoder = CSIEncoder(
            cfg.csi_channels,
            cfg.n_subcarriers,
            cfg.cnn_out_dim
        )

        self.beam_emb = BeamIDEncoder(
            cfg.n_beams_total,
            cfg.beam_emb_dim
        )

        policy_input_dim = (
            2 * cfg.cnn_out_dim +   # current + candidate pooled
            cfg.beam_emb_dim +      # previous beam
            1                        # RSS (optional)
        )

        self.lstm = nn.LSTM(
            input_size=policy_input_dim,
            hidden_size=cfg.lstm_hidden_dim,
            batch_first=True
        )

        self.fc = nn.Linear(cfg.lstm_hidden_dim, cfg.action_dim)

    def forward(
        self,
        rss,                # (B, T, 1)
        h_curr,             # (B, T, C_in, N_c)
        h_cand,             # (B, T, K, C_in, N_c)
        prev_beam_id        # (B, T)
    ):
        B, T, K = h_cand.shape[:3]

        # ===== Current beam CSI =====
        h_curr = h_curr.view(B*T, h_curr.size(2), h_curr.size(3))
        f_curr = self.csi_encoder(h_curr)
        f_curr = f_curr.view(B, T, -1)

        # ===== Candidate CSI =====
        h_cand = h_cand.view(B*T*K, h_cand.size(3), h_cand.size(4))
        f_cand = self.csi_encoder(h_cand)
        f_cand = f_cand.view(B, T, K, -1)

        # Pool candidate features
        f_cand_pool, _ = torch.max(f_cand, dim=2)  # (B, T, D_c)

        # ===== Previous beam embedding =====
        beam_emb = self.beam_emb(prev_beam_id)     # (B, T, D_b)

        # ===== Concatenate =====
        policy_input = torch.cat(
            [f_curr, f_cand_pool, beam_emb, rss],
            dim=-1
        )

        # ===== LSTM =====
        lstm_out, _ = self.lstm(policy_input)

        # ===== Action logits =====
        logits = self.fc(lstm_out)  # (B, T, K+1)

        return logits

class ValueNetwork(nn.Module):
    def __init__(self, cfg: BeamTrackingFeatureEncoder):
        super().__init__()

        self.beam_emb = BeamIDEncoder(
            cfg.n_beams_total,
            cfg.beam_emb_dim
        )

        value_input_dim = cfg.beam_emb_dim + 1  # beam + RSS

        self.lstm = nn.LSTM(
            input_size=value_input_dim,
            hidden_size=cfg.lstm_hidden_dim,
            batch_first=True
        )

        self.fc = nn.Linear(cfg.lstm_hidden_dim, 1)

    def forward(
        self,
        rss,           # (B, T, 1)
        prev_beam_id   # (B, T)
    ):
        beam_emb = self.beam_emb(prev_beam_id)   # (B, T, D_b)

        value_input = torch.cat([rss, beam_emb], dim=-1)

        lstm_out, _ = self.lstm(value_input)

        value = self.fc(lstm_out)  # (B, T, 1)

        return value

def value_loss_fn(value_pred, reward):
    reward = reward.unsqueeze(1)
    return F.mse_loss(value_pred, reward)

def policy_loss_with_baseline(dist, action, reward, value_pred):
    advantage = reward - value_pred.squeeze(1).detach()
    logp = dist.log_prob(action)
    return -(logp * advantage).mean()

class BeamTrackingAgent(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = BeamTrackingFeatureEncoder(K)
        self.backbone = BeamTrackingBackbone()
        self.policy = PolicyNetwork(k=K)
        self.value = ValueNetwork()

    def forward(self, obs_seq):
        """
        obs_seq: encoded feature sequence [B, T, 128]
        """
        h, _ = self.backbone(obs_seq)
        return self.policy(h), self.value(h)

def train_step(
    replay_buffer,
    batch_size,
    policy_net,
    value_net,
    policy_optimizer,
    value_optimizer,
    device='cuda'
):
    if len(replay_buffer) < batch_size:
        return

    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = replay_buffer.sample(batch_size)

    # 移動到device
    for k in obs_batch:
        obs_batch[k] = obs_batch[k].to(device)
        next_obs_batch[k] = next_obs_batch[k].to(device)
    action_batch = action_batch.to(device)
    reward_batch = reward_batch.to(device)

    # forward policy_net 輸出logits
    logits = policy_net(
        rss=obs_batch['rss_main'],
        h_curr=obs_batch['csi_main'],
        h_cand=obs_batch['candidate_csi'],
        prev_beam_id=obs_batch['prev_beam_id']
    )  # (B, T, action_dim)

    # 建立Categorical distribution
    dist = torch.distributions.Categorical(logits=logits)

    # forward value_net 得value預測
    value_pred = value_net(obs_batch)  # 假設你有設計value_net接obs

    # 計算loss
    p_loss = policy_loss_with_baseline(dist, action_batch, reward_batch, value_pred)
    v_loss = value_loss_fn(value_pred, reward_batch)

    # 更新value_net
    value_optimizer.zero_grad()
    v_loss.backward()
    value_optimizer.step()

    # 更新policy_net
    policy_optimizer.zero_grad()
    p_loss.backward()
    policy_optimizer.step()

class myEnv():
    def __init__(self, states,reward_table):
        self.cnt = 0
        self.all_states = states
        self.next_state_id = 0
        self.last_state_id = 0
        self.time_idx = 0
        self.curr_state = states[0,self.next_state_id,:]
        self.reward_table = torch.tensor(reward_table, device = 'cuda')
        self.last_state = states[0,self.next_state_id,:]
        self.time_count = 0
        self.action_space = torch.ones((20,), device = 'cuda')
    def step(self, action):
        # The state remain the same in this ver.
        reward = self.returnCapacity(self.next_state_id, action, self.time_idx)
        rand_s = torch.randint(0, 10000, (1,), device = 'cuda')
        self.last_state_id = self.next_state_id
        self.next_state_id = rand_s[0]

        self.time_idx = (self.time_idx+1)%20

        self.curr_state = self.all_states[self.time_idx,self.next_state_id,:]
        self.last_state = self.curr_state
        if (self.time_count)%5 ==4:
            self.action_space = torch.ones((20,), device = 'cuda')

        else:
            for i in range(20):
                if action[i]==1:
                    self.action_space[i] = 0
                self.curr_state[i] = self.curr_state[i]*self.action_space[i]
                self.curr_state[i+20] = self.curr_state[i]*self.action_space[i]
                self.curr_state[i+40] = self.curr_state[i]*self.action_space[i]

        self.time_count+=1
        next_state = self.curr_state
        #print(next_state[0:20])
        # Compute sum capacity for action

        #print(self.cnt)
        # Done whentrying 64 times
        # if self.cnt > 64:
        #     done = 1
        #     self.cnt = 0
        # else:
        #     done = 0
        if self.cnt>100:
            done = 1
        else:
            done = 0
        # Add count
        done = 1
        self.cnt += 1
        return next_state, reward, self.time_idx, done

    def reset(self):
        self.time_idx = -1
        self.curr_state = self.all_states[self.time_idx,self.next_state_id,:]
        self.cnt = 0
        time_vector = torch.zeros((5,), device = 'cuda')
        time_vector[self.time_idx] = 1

        beam_used = torch.zeros((20,), device = 'cuda')
        beam_used = self.curr_state[0:20]
        beam_used = (beam_used == 0)
        state = torch.cat((self.curr_state,beam_used), dim = 0)
        return torch.cat((state,time_vector), dim = 0)


    def returnCapacity(self, state, action, t):
        action_id = 0
        action_temp = torch.tensor([action[0:5],action[5:10],action[10:15],action[15:20]], device = 'cuda')
        for i in  range(4):
            if torch.sum(action_temp[i]) == 0:
                action_id += torch.pow(6,3-i)*5
            else:
                action_id += torch.pow(6,3-i)*torch.argmax(action_temp[i])
        throughput = self.reward_table[t,action_id,state]


        return throughput

num_episodes = 1000
batch_size = 32

# 參數設定
cfg = BeamTrackingFeatureEncoder()

# 建立ReplayBuffer
replay_buffer = ReplayBuffer(capacity=10000)

policy_net = PolicyNetwork(cfg)
value_net = ValueNetwork(cfg)
policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-4)
value_optimizer = torch.optim.Adam(value_net.parameters(), lr=5e-4)

for episode in range(num_episodes):
    obs = env.reset()
    done = False

    while not done:
        # 取得 policy logits
        with torch.no_grad():
            logits = policy_net(
                rss=obs['rss_main'].unsqueeze(0),   # 加 batch dim
                h_curr=obs['csi_main'].unsqueeze(0),
                h_cand=obs['candidate_csi'].unsqueeze(0),
                prev_beam_id=obs['prev_beam_id'].unsqueeze(0)
            )
            dist = torch.distributions.Categorical(logits=logits.squeeze(1))
            action = dist.sample().item()

        next_obs, reward, done, _ = env.step(action)

        # push到buffer
        replay_buffer.push(obs, torch.tensor(action), torch.tensor(reward), next_obs, torch.tensor(done))

        obs = next_obs

        # 訓練
        train_step(
            replay_buffer,
            batch_size,
            policy_net,
            value_net,
            policy_optimizer,
            value_optimizer,
            device='cuda'
        )